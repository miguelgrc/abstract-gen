{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9afaff48-b3c0-4fa3-a408-7378bff57132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n",
      "Python 3.12.3\n"
     ]
    }
   ],
   "source": [
    "!which python3\n",
    "!python3  --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "LO_HMcK4rG_C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "LO_HMcK4rG_C",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e0e2f086-63c0-438f-fa40-b161aa9efd39",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes==0.44.0 in /venv/main/lib/python3.12/site-packages (0.44.0)\n",
      "Requirement already satisfied: accelerate==1.1.1 in /venv/main/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: peft==0.14.0 in /venv/main/lib/python3.12/site-packages (0.14.0)\n",
      "Requirement already satisfied: transformers==4.46.3 in /venv/main/lib/python3.12/site-packages (4.46.3)\n",
      "Requirement already satisfied: torch==2.5.1 in /venv/main/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: datasets==3.6.0 in /venv/main/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: evaluate in /venv/main/lib/python3.12/site-packages (0.4.5)\n",
      "Requirement already satisfied: rouge_score in /venv/main/lib/python3.12/site-packages (0.1.2)\n",
      "Requirement already satisfied: aiohttp in /venv/main/lib/python3.12/site-packages (3.12.15)\n",
      "Requirement already satisfied: nltk in /venv/main/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.12/site-packages (from bitsandbytes==0.44.0) (2.3.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /venv/main/lib/python3.12/site-packages (from accelerate==1.1.1) (0.33.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.12/site-packages (from accelerate==1.1.1) (25.0)\n",
      "Requirement already satisfied: psutil in /venv/main/lib/python3.12/site-packages (from accelerate==1.1.1) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /venv/main/lib/python3.12/site-packages (from accelerate==1.1.1) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /venv/main/lib/python3.12/site-packages (from accelerate==1.1.1) (0.6.2)\n",
      "Requirement already satisfied: tqdm in /venv/main/lib/python3.12/site-packages (from peft==0.14.0) (4.67.1)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from transformers==4.46.3) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /venv/main/lib/python3.12/site-packages (from transformers==4.46.3) (2025.9.1)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.12/site-packages (from transformers==4.46.3) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /venv/main/lib/python3.12/site-packages (from transformers==4.46.3) (0.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (4.14.0)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /venv/main/lib/python3.12/site-packages (from torch==2.5.1) (1.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /venv/main/lib/python3.12/site-packages (from datasets==3.6.0) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /venv/main/lib/python3.12/site-packages (from datasets==3.6.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /venv/main/lib/python3.12/site-packages (from datasets==3.6.0) (2.3.2)\n",
      "Requirement already satisfied: xxhash in /venv/main/lib/python3.12/site-packages (from datasets==3.6.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /venv/main/lib/python3.12/site-packages (from datasets==3.6.0) (0.70.16)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate==1.1.1) (1.1.5)\n",
      "Requirement already satisfied: absl-py in /venv/main/lib/python3.12/site-packages (from rouge_score) (2.3.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /venv/main/lib/python3.12/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /venv/main/lib/python3.12/site-packages (from aiohttp) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /venv/main/lib/python3.12/site-packages (from aiohttp) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /venv/main/lib/python3.12/site-packages (from aiohttp) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /venv/main/lib/python3.12/site-packages (from aiohttp) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /venv/main/lib/python3.12/site-packages (from aiohttp) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /venv/main/lib/python3.12/site-packages (from aiohttp) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /venv/main/lib/python3.12/site-packages (from aiohttp) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /venv/main/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)\n",
      "Requirement already satisfied: click in /venv/main/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /venv/main/lib/python3.12/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /venv/main/lib/python3.12/site-packages (from requests->transformers==4.46.3) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.12/site-packages (from requests->transformers==4.46.3) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.12/site-packages (from requests->transformers==4.46.3) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2->torch==2.5.1) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.12/site-packages (from pandas->datasets==3.6.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /venv/main/lib/python3.12/site-packages (from pandas->datasets==3.6.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /venv/main/lib/python3.12/site-packages (from pandas->datasets==3.6.0) (2025.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install bitsandbytes==0.44.0 accelerate==1.1.1 peft==0.14.0 transformers==4.46.3 torch==2.5.1 datasets==3.6.0 evaluate rouge_score aiohttp\n",
    "# Need old datasets version due to incompatible dataset format (.py script)\n",
    "# Need old transformers due to `AttributeError: 'DynamicCache' object has no attribute 'seen_tokens'` on new versions\n",
    "# Need old accelerate to avoid issues with quantization failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cf9007b",
   "metadata": {
    "id": "5cf9007b"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import AutoTokenizer\n",
    "from aiohttp import ClientTimeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ed0f80-2c6c-40a1-95fb-f9c842234427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733295e901aa45229734aeb03c0a6ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f20f6c7-1339-4985-8f89-5acc79a1e48b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_5k_sp = load_from_disk(\"/workspace/tokenized_5k_sp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9e6993e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9e6993e",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "01e91bd9-e151-468b-dd5b-c9ec9628d98e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52945\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# print(json.dumps(tokenized_5k_sp['train'][:5], indent=1))\n",
    "print(len(tokenized_5k_sp[\"train\"])) # 52945k entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482bc4d1",
   "metadata": {
    "id": "482bc4d1"
   },
   "outputs": [],
   "source": [
    "prefix = \"generate a summary that will serve as abstract for the following scientific paper:\\n\\n\"\n",
    "\n",
    "def preprocess(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128000, truncation=True) # max_length here could be reduced to 5000 and truncation disabled\n",
    "\n",
    "    labels = tokenizer(examples[\"abstract\"], max_length=256, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb0a6505",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "904a256fd5ab473d97a3a4a5c0ea158c",
      "be24a2d912dd4f73a6eac62aaaa4b650",
      "2210239dd2224be58b81e293c674c12f",
      "fd66ac3a4e2c4c75a8a06f6acfe1662f",
      "95599445173345338dfcf9adc8c4cee0",
      "3d2ffdf5f59a47fbb4435c4b8e918261",
      "f670977056df4ac38d5f698d9ac26764",
      "f25d707a1108402db167ac67361fcc55",
      "2a5e7e178a8a43c7b5165897cad0a610",
      "074f48956e9b4d6d953969ca353f795d",
      "f36d392e748347d5acb87be1bc8b7186"
     ]
    },
    "id": "bb0a6505",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "9aeb0ce6-43b6-4ced-9650-55245c490f19"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ef22047a6849e191d4d0696adab21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "# Define the path in Google Drive\n",
    "drive_path = \"/content/drive/MyDrive/Colab Notebooks/abstract-gen/tokenized_sp\"\n",
    "\n",
    "if os.path.exists(drive_path):\n",
    "  tokenized_sp = load_from_disk(drive_path)\n",
    "else:\n",
    "  tokenized_sp = sp.map(preprocess, batched=True)\n",
    "  tokenized_sp.save_to_disk(drive_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b27fb65c",
   "metadata": {
    "id": "b27fb65c",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2331bd0406486c9bd101641c97f5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred # Each pred in preds is an array of token ids\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count.nonzero(pred != tokenizer.pad_token_id) for pred in preds]  # Generates a boolean array for each pred (!= in np works element-wise in this case)\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3ee04d-7abd-41a2-8aea-286abbf4d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install peft==0.44.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672e6b77-dcea-435d-a9ec-6b680f83c2d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import importlib\n",
    "\n",
    "# import numpy\n",
    "# importlib.reload(numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "296d9833",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "f3b3901214d346f88c1a91e7e38fe660",
      "52267788a93a493e8b2b83cf6ed30e95",
      "9685442264e440f0bdc28f4782941587",
      "4f11be13010749e98d23ad701cb9d7fc",
      "a9412c0c1980445bbb38ed40b4c7dbed",
      "dd545979c96e4392b0693f910f62ade2",
      "c4bc7f30f36e4186bbcb44ae7681cd20",
      "ded3180a8f81466c9ebf03c3f6b369d4",
      "a5be1b54a6b048c09dd37d3fa7f14f72",
      "a220894313684e1dbd0e18b37469d2ab",
      "b90789bc945b4417beaa19294b8a9aab"
     ]
    },
    "id": "296d9833",
    "outputId": "bb938c9f-1acd-47cd-e1e3-cde11834c226"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1ff32df0104add9c30197452078711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_storage=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Quantized model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        checkpoint,\n",
    "        # return_dict=True,\n",
    "        # low_cpu_mem_usage=True,\n",
    "        # torch_dtype=\"auto\",\n",
    "        # device_map=\"auto\",\n",
    "        # trust_remote_code=True,\n",
    "        quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UiPIVzYch_wT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UiPIVzYch_wT",
    "outputId": "025a15cf-c389-49af-e05c-a3a6e8f3fbe4"
   },
   "outputs": [],
   "source": [
    "summ = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "gen_args = {\n",
    "  \"max_new_tokens\": 256,\n",
    "  \"return_full_text\": False,\n",
    "  \"temperature\": 0.0,\n",
    "  \"do_sample\": False,\n",
    "}\n",
    "\n",
    "# summ(\"generate a summary that will serve as abstract for the following scientific paper:\\n\\n\" + sp['train'][0]['article'], **gen_args)\n",
    "print(summ(\"generate a summary:\\n\\n\" + \"Microsoft has joined the competitive landscape of large language models alongside Meta AI with the introduction of the Phi-3.5 series. This series includes a small language model, a vision language model, and employs a Mixture-of-Experts approach to achieve top-tier performance. In this tutorial, we will explore the Microsoft Phi-3.5 family of models. We will load the Phi-3.5-mini-instruct model and fine-tune it to classify e-commerce products based on their text descriptions. In the final steps, we will demonstrate how to merge the LoRA (Low-Rank Adaptation) with the base model and push it to Hugging Face. This will enable efficient cloud deployment, making the model accessible for various applications.\", **gen_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "oeIq7KPgmY4J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "editable": true,
    "id": "oeIq7KPgmY4J",
    "outputId": "3b3f2687-9bad-487c-d0b9-bbaf98663e6d",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft: 0.14.0\n",
      "trainable params: 37,748,736 || all params: 3,858,828,288 || trainable%: 0.9782\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "import peft\n",
    "print(\"peft:\", peft.__version__)\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"o_proj\", \"qkv_proj\"] # Check model arch to see how it has these two layers\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8b034619-aaf5-447d-a362-0ee013f93e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sample[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7767a8-65c9-4926-9bb3-9f7ef07363c2",
   "metadata": {},
   "source": [
    "# Encoder-decoder (BART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a94b8d1e",
   "metadata": {
    "id": "a94b8d1e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from aiohttp import ClientTimeout\n",
    "import os\n",
    "\n",
    "drive_path_raw_dataset = \"/workspace/scientific_papers_hf\"\n",
    "\n",
    "if os.path.exists(drive_path_raw_dataset):\n",
    "  sp = load_from_disk(drive_path_raw_dataset)\n",
    "else:\n",
    "  notebook_login()\n",
    "  sp = load_dataset(\"armanc/scientific_papers\", \"arxiv\", trust_remote_code=True, storage_options={'client_kwargs': {'timeout': ClientTimeout(total=3600)}})\n",
    "  sp.save_to_disk(drive_path_raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9eef489-ffa9-4e54-a3c5-a3b079e9676b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4770d2165c0e49589833863747abc3d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/203037 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6e8ec62cc24788ac6e1d0ae2fddced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6436 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865e0dfcba924b229f14026ed31c6f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sp_small = sp.filter(lambda ex: len(ex[\"article\"]) > 1000 and len(ex[\"article\"]) < 15000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46af565d",
   "metadata": {
    "id": "46af565d"
   },
   "outputs": [],
   "source": [
    "checkpoint = \"allenai/led-base-16384\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6893e632-468e-4b24-adc6-c8888620cce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'I', 'Ġloved', 'Ġreading', 'Ġthe', 'ĠHunger', 'ĠGames', '!', '</s>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer(\"I loved reading the Hunger Games!\").input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "293a3853-6657-495f-935e-c70d47d51b05",
   "metadata": {
    "id": "482bc4d1"
   },
   "outputs": [],
   "source": [
    "max_input_length = 5120\n",
    "max_target_length = 256\n",
    "\n",
    "def preprocess(examples):\n",
    "    model_inputs = tokenizer(examples[\"article\"], max_length=max_input_length, truncation=True)\n",
    "\n",
    "    labels = tokenizer(examples[\"abstract\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0dc6732-8711-48f4-84a7-7a3775d7cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "path = \"/workspace/tokenized_sp_small\"\n",
    "\n",
    "if os.path.exists(path):\n",
    "  tokenized_sp_small = load_from_disk(path)\n",
    "else:\n",
    "  tokenized_sp_small = sp_small.map(preprocess, batched=True)\n",
    "  tokenized_sp_small.save_to_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86ebc06a-bf0c-4a16-ba1e-06ea1a2ce8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4599\n",
      "2291\n",
      "2832\n",
      "959\n",
      "1740\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(len(tokenized_sp_small[\"train\"][i][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4pFBgFlzjZR1",
   "metadata": {
    "id": "4pFBgFlzjZR1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "batch_size = 6\n",
    "epochs = 2\n",
    "\n",
    "# KIND OF UNUSED with accelerate\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"led-abstract-gen\",\n",
    "    eval_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    # gradient_accumulation_steps=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3, # Only save up to 3 checkpoints (hard drive space savings)\n",
    "    num_train_epochs=epochs,\n",
    "    predict_with_generate=True, # Evaluate model based on generated summaries\n",
    "    fp16=True,\n",
    "    # optim=\"adafactor\",\n",
    "    push_to_hub=True, #\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "a8def9d5-6e47-4fed-876e-4e5f91bf818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNUSED with accelerate\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred # Each pred in preds is an array of token ids\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True) # Decode generated summaries into text\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id) # Can't decode -100 in labels so we replace them\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True) # Decode reference summaries into text\n",
    "\n",
    "    # ROUGE expects a new line after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    # prediction_lens = [np.count.nonzero(pred != tokenizer.pad_token_id) for pred in preds]  # Generates a boolean array for each pred (!= in np works element-wise in this case)\n",
    "    # result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()} # Extract median scoress\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "462032f1",
   "metadata": {
    "id": "462032f1"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# Also shifts the the labels to the right by one during decoding so that the decoder\n",
    "# only sees the previous labels and can't cheat. This is necessary for encoder-decoder models\n",
    "# The pad_to_multiple_of is used to prevent annoying warnings at training time\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, pad_to_multiple_of=model.config.attention_window[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "549aab90-3c7f-4303-a31b-909fc19cd3f6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column name ['section_names', 'abstract', 'article'] not in the dataset. Current columns in the dataset: ['input_ids', 'attention_mask', 'labels']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# We need to remove the string columns (only leave the vector ones) otherwise we will get an error in the data collator\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# More info: https://huggingface.co/learn/llm-course/chapter7/5#fine-tuning-mt5-with-the-trainer-api\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m tokenized_sp_small = \u001b[43mtokenized_sp_small\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43msp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Equivalent to [\"article\", \"abstract\", \"section_names\"]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/datasets/dataset_dict.py:373\u001b[39m, in \u001b[36mDatasetDict.remove_columns\u001b[39m\u001b[34m(self, column_names)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[33;03mRemove one or several column(s) from each split in the dataset\u001b[39;00m\n\u001b[32m    336\u001b[39m \u001b[33;03mand the features associated to the column(s).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    370\u001b[39m \u001b[33;03m```\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;28mself\u001b[39m._check_values_type()\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict({k: \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/datasets/arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/datasets/fingerprint.py:442\u001b[39m, in \u001b[36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[32m    440\u001b[39m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/datasets/arrow_dataset.py:2161\u001b[39m, in \u001b[36mDataset.remove_columns\u001b[39m\u001b[34m(self, column_names, new_fingerprint)\u001b[39m\n\u001b[32m   2159\u001b[39m missing_columns = \u001b[38;5;28mset\u001b[39m(column_names) - \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m._data.column_names)\n\u001b[32m   2160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing_columns:\n\u001b[32m-> \u001b[39m\u001b[32m2161\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2162\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(missing_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in the dataset. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset._data.column_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2164\u001b[39m     )\n\u001b[32m   2166\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m column_names:\n\u001b[32m   2167\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m dataset._info.features[column_name]\n",
      "\u001b[31mValueError\u001b[39m: Column name ['section_names', 'abstract', 'article'] not in the dataset. Current columns in the dataset: ['input_ids', 'attention_mask', 'labels']"
     ]
    }
   ],
   "source": [
    "# We need to remove the string columns (only leave the vector ones) otherwise we will get an error in the data collator\n",
    "# More info: https://huggingface.co/learn/llm-course/chapter7/5#fine-tuning-mt5-with-the-trainer-api\n",
    "tokenized_sp_small = tokenized_sp_small.remove_columns(sp[\"train\"].column_names) # Equivalent to [\"article\", \"abstract\", \"section_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a28007f-b0bf-4710-b352-1eaa4de619a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_collator([tokenized_sp_small[\"train\"][i] for i in range(2)])[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "e4786b57-b5fb-423a-8661-40a91d9b704e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# UNUSED with accelerate\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_sp_small[\"train\"],\n",
    "    eval_dataset=tokenized_sp_small[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c234bcb-cbdc-42c1-976e-ca3af02a2c23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_storage=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# Quantized model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        checkpoint,\n",
    "        # return_dict=True,\n",
    "        # low_cpu_mem_usage=True,\n",
    "        # torch_dtype=\"auto\",\n",
    "        # device_map=\"auto\",\n",
    "        # trust_remote_code=True,\n",
    "        # quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7fb6188-fce6-4468-9b03-3cb390641f31",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <class 'transformers.models.led.modeling_led.LEDForConditionalGeneration'>\n",
      "led <class 'transformers.models.led.modeling_led.LEDModel'>\n",
      "led.shared <class 'torch.nn.modules.sparse.Embedding'>\n",
      "led.encoder <class 'transformers.models.led.modeling_led.LEDEncoder'>\n",
      "led.encoder.embed_positions <class 'transformers.models.led.modeling_led.LEDLearnedPositionalEmbedding'>\n",
      "led.encoder.layers <class 'torch.nn.modules.container.ModuleList'>\n",
      "led.encoder.layers.0 <class 'transformers.models.led.modeling_led.LEDEncoderLayer'>\n",
      "led.encoder.layers.0.self_attn <class 'transformers.models.led.modeling_led.LEDEncoderAttention'>\n",
      "led.encoder.layers.0.self_attn.longformer_self_attn <class 'transformers.models.led.modeling_led.LEDEncoderSelfAttention'>\n",
      "led.encoder.layers.0.self_attn.longformer_self_attn.query <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.0.self_attn.longformer_self_attn.key <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.0.self_attn.longformer_self_attn.value <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.0.self_attn.longformer_self_attn.query_global <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.0.self_attn.longformer_self_attn.key_global <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.0.self_attn.longformer_self_attn.value_global <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.0.self_attn.output <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.0.self_attn_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.encoder.layers.0.activation_fn <class 'transformers.activations.GELUActivation'>\n",
      "led.encoder.layers.0.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.0.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.0.final_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.encoder.layers.1 <class 'transformers.models.led.modeling_led.LEDEncoderLayer'>\n",
      "led.encoder.layers.1.self_attn <class 'transformers.models.led.modeling_led.LEDEncoderAttention'>\n",
      "led.encoder.layers.1.self_attn.longformer_self_attn <class 'transformers.models.led.modeling_led.LEDEncoderSelfAttention'>\n",
      "led.encoder.layers.1.self_attn.longformer_self_attn.query <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.1.self_attn.longformer_self_attn.key <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.1.self_attn.longformer_self_attn.value <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.1.self_attn.longformer_self_attn.query_global <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.1.self_attn.longformer_self_attn.key_global <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.1.self_attn.longformer_self_attn.value_global <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.1.self_attn.output <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.1.self_attn_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.encoder.layers.1.activation_fn <class 'transformers.activations.GELUActivation'>\n",
      "led.encoder.layers.1.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.1.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.1.final_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.encoder.layers.2 <class 'transformers.models.led.modeling_led.LEDEncoderLayer'>\n",
      "led.encoder.layers.2.self_attn <class 'transformers.models.led.modeling_led.LEDEncoderAttention'>\n",
      "led.encoder.layers.2.self_attn.longformer_self_attn <class 'transformers.models.led.modeling_led.LEDEncoderSelfAttention'>\n",
      "led.encoder.layers.2.self_attn.longformer_self_attn.query <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.2.self_attn.longformer_self_attn.key <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.2.self_attn.longformer_self_attn.value <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.2.self_attn.longformer_self_attn.query_global <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.2.self_attn.longformer_self_attn.key_global <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.2.self_attn.longformer_self_attn.value_global <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.2.self_attn.output <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.2.self_attn_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.encoder.layers.2.activation_fn <class 'transformers.activations.GELUActivation'>\n",
      "led.encoder.layers.2.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.2.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.2.final_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.encoder.layers.3 <class 'transformers.models.led.modeling_led.LEDEncoderLayer'>\n",
      "led.encoder.layers.3.self_attn <class 'transformers.models.led.modeling_led.LEDEncoderAttention'>\n",
      "led.encoder.layers.3.self_attn.longformer_self_attn <class 'transformers.models.led.modeling_led.LEDEncoderSelfAttention'>\n",
      "led.encoder.layers.3.self_attn.longformer_self_attn.query <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.3.self_attn.longformer_self_attn.key <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.3.self_attn.longformer_self_attn.value <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.3.self_attn.longformer_self_attn.query_global <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.3.self_attn.longformer_self_attn.key_global <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.3.self_attn.longformer_self_attn.value_global <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.3.self_attn.output <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.3.self_attn_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.encoder.layers.3.activation_fn <class 'transformers.activations.GELUActivation'>\n",
      "led.encoder.layers.3.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.3.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.3.final_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.encoder.layers.4 <class 'transformers.models.led.modeling_led.LEDEncoderLayer'>\n",
      "led.encoder.layers.4.self_attn <class 'transformers.models.led.modeling_led.LEDEncoderAttention'>\n",
      "led.encoder.layers.4.self_attn.longformer_self_attn <class 'transformers.models.led.modeling_led.LEDEncoderSelfAttention'>\n",
      "led.encoder.layers.4.self_attn.longformer_self_attn.query <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.4.self_attn.longformer_self_attn.key <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.4.self_attn.longformer_self_attn.value <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.4.self_attn.longformer_self_attn.query_global <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.4.self_attn.longformer_self_attn.key_global <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.4.self_attn.longformer_self_attn.value_global <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.4.self_attn.output <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.4.self_attn_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.encoder.layers.4.activation_fn <class 'transformers.activations.GELUActivation'>\n",
      "led.encoder.layers.4.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.4.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.4.final_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.encoder.layers.5 <class 'transformers.models.led.modeling_led.LEDEncoderLayer'>\n",
      "led.encoder.layers.5.self_attn <class 'transformers.models.led.modeling_led.LEDEncoderAttention'>\n",
      "led.encoder.layers.5.self_attn.longformer_self_attn <class 'transformers.models.led.modeling_led.LEDEncoderSelfAttention'>\n",
      "led.encoder.layers.5.self_attn.longformer_self_attn.query <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.5.self_attn.longformer_self_attn.key <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.5.self_attn.longformer_self_attn.value <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.5.self_attn.longformer_self_attn.query_global <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.5.self_attn.longformer_self_attn.key_global <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.5.self_attn.longformer_self_attn.value_global <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.5.self_attn.output <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.5.self_attn_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.encoder.layers.5.activation_fn <class 'transformers.activations.GELUActivation'>\n",
      "led.encoder.layers.5.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.5.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.encoder.layers.5.final_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.encoder.layernorm_embedding <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder <class 'transformers.models.led.modeling_led.LEDDecoder'>\n",
      "led.decoder.embed_positions <class 'transformers.models.led.modeling_led.LEDLearnedPositionalEmbedding'>\n",
      "led.decoder.layers <class 'torch.nn.modules.container.ModuleList'>\n",
      "led.decoder.layers.0 <class 'transformers.models.led.modeling_led.LEDDecoderLayer'>\n",
      "led.decoder.layers.0.self_attn <class 'transformers.models.led.modeling_led.LEDDecoderAttention'>\n",
      "led.decoder.layers.0.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.0.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.0.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.0.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.0.activation_fn <class 'transformers.activations.GELUActivation'>\n",
      "led.decoder.layers.0.self_attn_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder.layers.0.encoder_attn <class 'transformers.models.led.modeling_led.LEDDecoderAttention'>\n",
      "led.decoder.layers.0.encoder_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.0.encoder_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.0.encoder_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.0.encoder_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.0.encoder_attn_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder.layers.0.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.0.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.0.final_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder.layers.1 <class 'transformers.models.led.modeling_led.LEDDecoderLayer'>\n",
      "led.decoder.layers.1.self_attn <class 'transformers.models.led.modeling_led.LEDDecoderAttention'>\n",
      "led.decoder.layers.1.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.1.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.1.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.1.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.1.activation_fn <class 'transformers.activations.GELUActivation'>\n",
      "led.decoder.layers.1.self_attn_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder.layers.1.encoder_attn <class 'transformers.models.led.modeling_led.LEDDecoderAttention'>\n",
      "led.decoder.layers.1.encoder_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.1.encoder_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.1.encoder_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.1.encoder_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.1.encoder_attn_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder.layers.1.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.1.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.1.final_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder.layers.2 <class 'transformers.models.led.modeling_led.LEDDecoderLayer'>\n",
      "led.decoder.layers.2.self_attn <class 'transformers.models.led.modeling_led.LEDDecoderAttention'>\n",
      "led.decoder.layers.2.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.2.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.2.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.2.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.2.activation_fn <class 'transformers.activations.GELUActivation'>\n",
      "led.decoder.layers.2.self_attn_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder.layers.2.encoder_attn <class 'transformers.models.led.modeling_led.LEDDecoderAttention'>\n",
      "led.decoder.layers.2.encoder_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.2.encoder_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.2.encoder_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.2.encoder_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.2.encoder_attn_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder.layers.2.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.2.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.2.final_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder.layers.3 <class 'transformers.models.led.modeling_led.LEDDecoderLayer'>\n",
      "led.decoder.layers.3.self_attn <class 'transformers.models.led.modeling_led.LEDDecoderAttention'>\n",
      "led.decoder.layers.3.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.3.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.3.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.3.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.3.activation_fn <class 'transformers.activations.GELUActivation'>\n",
      "led.decoder.layers.3.self_attn_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder.layers.3.encoder_attn <class 'transformers.models.led.modeling_led.LEDDecoderAttention'>\n",
      "led.decoder.layers.3.encoder_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.3.encoder_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.3.encoder_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.3.encoder_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.3.encoder_attn_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder.layers.3.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.3.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.3.final_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder.layers.4 <class 'transformers.models.led.modeling_led.LEDDecoderLayer'>\n",
      "led.decoder.layers.4.self_attn <class 'transformers.models.led.modeling_led.LEDDecoderAttention'>\n",
      "led.decoder.layers.4.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.4.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.4.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.4.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.4.activation_fn <class 'transformers.activations.GELUActivation'>\n",
      "led.decoder.layers.4.self_attn_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder.layers.4.encoder_attn <class 'transformers.models.led.modeling_led.LEDDecoderAttention'>\n",
      "led.decoder.layers.4.encoder_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.4.encoder_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.4.encoder_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.4.encoder_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.4.encoder_attn_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder.layers.4.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.4.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.4.final_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder.layers.5 <class 'transformers.models.led.modeling_led.LEDDecoderLayer'>\n",
      "led.decoder.layers.5.self_attn <class 'transformers.models.led.modeling_led.LEDDecoderAttention'>\n",
      "led.decoder.layers.5.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.5.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.5.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.5.self_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.5.activation_fn <class 'transformers.activations.GELUActivation'>\n",
      "led.decoder.layers.5.self_attn_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder.layers.5.encoder_attn <class 'transformers.models.led.modeling_led.LEDDecoderAttention'>\n",
      "led.decoder.layers.5.encoder_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.5.encoder_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.5.encoder_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.5.encoder_attn.out_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.5.encoder_attn_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder.layers.5.fc1 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.5.fc2 <class 'torch.nn.modules.linear.Linear'>\n",
      "led.decoder.layers.5.final_layer_norm <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "led.decoder.layernorm_embedding <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "lm_head <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name, type(module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7888e75d-1672-48d1-967f-ecd3c7dedd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bec75b9f-b875-49bd-b85f-fc1ddddec77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1ca46c2-0ff4-4cec-90ad-2ce49af3be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sp_small.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8aa505e9-acaa-46b3-b5b6-c6d29041a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = epochs * num_update_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "49b65f76-c45f-4198-aa7d-60de87ba3c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c876221-ff5f-44ea-8260-5e875347197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a notable reduction in VRAM usage by using adafactor vs adam\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "\n",
    "optimizer = Adafactor(model.parameters(), scale_parameter=False, relative_step=False, lr=1e-3) # Not sure about these settings\n",
    "lr_scheduler = AdafactorSchedule(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef1be552-d5bf-45d0-9531-1d5aefad661e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "batch_size = training_args.per_device_train_batch_size\n",
    "gradient_accumulation_steps = training_args.gradient_accumulation_steps\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_sp_small[\"train\"], batch_size=batch_size, collate_fn=data_collator, shuffle=True)\n",
    "\n",
    "eval_dataloader = DataLoader(tokenized_sp_small[\"validation\"], batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "if training_args.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\", gradient_accumulation_steps=gradient_accumulation_steps)\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
    "\n",
    "# model.train()\n",
    "\n",
    "# for step, batch in enumerate(dataloader, start=1):\n",
    "#     loss = model(**batch).loss\n",
    "#     loss = loss / training_args.gradient_accumulation_steps\n",
    "#     accelerator.backward(loss)\n",
    "#     if step % training_args.gradient_accumulation_steps == 0:\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95fd269a-58fb-4554-8f71-0cf35c1d3f9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'miguelgrc/led-abstract-gen'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import get_full_repo_name\n",
    "\n",
    "model_name = \"led-abstract-gen\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b797d6e6-6c01-4738-ba0b-f0eb33c5c080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/miguelgrc/led-abstract-gen', endpoint='https://huggingface.co', repo_type='model', repo_id='miguelgrc/led-abstract-gen')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "create_repo(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d1ef8fd-e848-4013-88b1-79156f5e7771",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m batch = {k: v.cpu() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch.items()}\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(out.loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/accelerate/utils/operations.py:823\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/accelerate/utils/operations.py:811\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    810\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/models/led/modeling_led.py:2398\u001b[39m, in \u001b[36mLEDForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, global_attention_mask, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   2393\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2394\u001b[39m         decoder_input_ids = shift_tokens_right(\n\u001b[32m   2395\u001b[39m             labels, \u001b[38;5;28mself\u001b[39m.config.pad_token_id, \u001b[38;5;28mself\u001b[39m.config.decoder_start_token_id\n\u001b[32m   2396\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2398\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mled\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2399\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2409\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2410\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2411\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2412\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2413\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2414\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2415\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2416\u001b[39m lm_logits = \u001b[38;5;28mself\u001b[39m.lm_head(outputs[\u001b[32m0\u001b[39m]) + \u001b[38;5;28mself\u001b[39m.final_logits_bias\n\u001b[32m   2418\u001b[39m masked_lm_loss = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/models/led/modeling_led.py:2248\u001b[39m, in \u001b[36mLEDModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, global_attention_mask, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   2243\u001b[39m     decoder_input_ids = shift_tokens_right(\n\u001b[32m   2244\u001b[39m         input_ids, \u001b[38;5;28mself\u001b[39m.config.pad_token_id, \u001b[38;5;28mself\u001b[39m.config.decoder_start_token_id\n\u001b[32m   2245\u001b[39m     )\n\u001b[32m   2247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2248\u001b[39m     encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2253\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2254\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2255\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2258\u001b[39m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a LEDEncoderBaseModelOutput when return_dict=False\u001b[39;00m\n\u001b[32m   2259\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, LEDEncoderBaseModelOutput):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/models/led/modeling_led.py:1866\u001b[39m, in \u001b[36mLEDEncoder.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, global_attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1855\u001b[39m         layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1856\u001b[39m             encoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m   1857\u001b[39m             hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1863\u001b[39m             output_attentions,\n\u001b[32m   1864\u001b[39m         )\n\u001b[32m   1865\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1866\u001b[39m         layer_outputs = \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1867\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1868\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1869\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1870\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1871\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1872\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1873\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1874\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1875\u001b[39m     hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1877\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m   1878\u001b[39m     \u001b[38;5;66;03m# bzs x seq_len x num_attn_heads x (num_global_attn + attention_window_len + 1) => bzs x num_attn_heads x seq_len x (num_global_attn + attention_window_len + 1)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/models/led/modeling_led.py:952\u001b[39m, in \u001b[36mLEDEncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[39m\n\u001b[32m    943\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    945\u001b[39m \u001b[33;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape *(batch, seq_len, embed_dim)*\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    949\u001b[39m \u001b[33;03m        *(encoder_attention_heads,)*.\u001b[39;00m\n\u001b[32m    950\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    951\u001b[39m residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m952\u001b[39m attn_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    961\u001b[39m hidden_states = attn_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    962\u001b[39m hidden_states = nn.functional.dropout(hidden_states, p=\u001b[38;5;28mself\u001b[39m.dropout, training=\u001b[38;5;28mself\u001b[39m.training)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/models/led/modeling_led.py:761\u001b[39m, in \u001b[36mLEDEncoderAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    750\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    751\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    757\u001b[39m     output_attentions: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    758\u001b[39m ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n\u001b[32m    759\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Input shape: Batch x Time x Channel\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m761\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlongformer_self_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    771\u001b[39m     attn_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m])\n\u001b[32m    772\u001b[39m     outputs = (attn_output,) + self_outputs[\u001b[32m1\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/models/led/modeling_led.py:182\u001b[39m, in \u001b[36mLEDEncoderSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[39m\n\u001b[32m    179\u001b[39m query_vectors = query_vectors.view(seq_len, batch_size, \u001b[38;5;28mself\u001b[39m.num_heads, \u001b[38;5;28mself\u001b[39m.head_dim).transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    180\u001b[39m key_vectors = key_vectors.view(seq_len, batch_size, \u001b[38;5;28mself\u001b[39m.num_heads, \u001b[38;5;28mself\u001b[39m.head_dim).transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m attn_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sliding_chunks_query_key_matmul\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mone_sided_attn_window_size\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# values to pad for attention probs\u001b[39;00m\n\u001b[32m    187\u001b[39m remove_from_windowed_attention_mask = (attention_mask != \u001b[32m0\u001b[39m)[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/models/led/modeling_led.py:466\u001b[39m, in \u001b[36mLEDEncoderSelfAttention._sliding_chunks_query_key_matmul\u001b[39m\u001b[34m(self, query, key, window_overlap)\u001b[39m\n\u001b[32m    457\u001b[39m diagonal_chunked_attention_scores = \u001b[38;5;28mself\u001b[39m._pad_and_transpose_last_two_dims(\n\u001b[32m    458\u001b[39m     diagonal_chunked_attention_scores, padding=(\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    459\u001b[39m )\n\u001b[32m    461\u001b[39m \u001b[38;5;66;03m# allocate space for the overall attention matrix where the chunks are combined. The last dimension\u001b[39;00m\n\u001b[32m    462\u001b[39m \u001b[38;5;66;03m# has (window_overlap * 2 + 1) columns. The first (window_overlap) columns are the window_overlap lower triangles (attention from a word to\u001b[39;00m\n\u001b[32m    463\u001b[39m \u001b[38;5;66;03m# window_overlap previous words). The following column is attention score from each word to itself, then\u001b[39;00m\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# followed by window_overlap columns for the upper triangle.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m diagonal_attention_scores = \u001b[43mdiagonal_chunked_attention_scores\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks_count\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_overlap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_overlap\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;66;03m# copy parts from diagonal_chunked_attention_scores into the combined matrix of attentions\u001b[39;00m\n\u001b[32m    471\u001b[39m \u001b[38;5;66;03m# - copying the main diagonal and the upper triangle\u001b[39;00m\n\u001b[32m    472\u001b[39m diagonal_attention_scores[:, :-\u001b[32m1\u001b[39m, :, window_overlap:] = diagonal_chunked_attention_scores[\n\u001b[32m    473\u001b[39m     :, :, :window_overlap, : window_overlap + \u001b[32m1\u001b[39m\n\u001b[32m    474\u001b[39m ]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/IPython/core/async_helpers.py:128\u001b[39m, in \u001b[36m_pseudo_sync_runner\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3386\u001b[39m, in \u001b[36mInteractiveShell.run_cell_async\u001b[39m\u001b[34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[39m\n\u001b[32m   3382\u001b[39m exec_count = \u001b[38;5;28mself\u001b[39m.execution_count\n\u001b[32m   3383\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.error_in_exec:\n\u001b[32m   3384\u001b[39m     \u001b[38;5;66;03m# Store formatted traceback and error details\u001b[39;00m\n\u001b[32m   3385\u001b[39m     \u001b[38;5;28mself\u001b[39m.history_manager.exceptions[exec_count] = (\n\u001b[32m-> \u001b[39m\u001b[32m3386\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_exception_for_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror_in_exec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3387\u001b[39m     )\n\u001b[32m   3389\u001b[39m \u001b[38;5;66;03m# Each cell is a *single* input, regardless of how many lines it has\u001b[39;00m\n\u001b[32m   3390\u001b[39m \u001b[38;5;28mself\u001b[39m.execution_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3440\u001b[39m, in \u001b[36mInteractiveShell._format_exception_for_storage\u001b[39m\u001b[34m(self, exception, filename, running_compiled_code)\u001b[39m\n\u001b[32m   3437\u001b[39m         stb = evalue._render_traceback_()\n\u001b[32m   3438\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3439\u001b[39m         \u001b[38;5;66;03m# Otherwise, use InteractiveTB to format the traceback.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3440\u001b[39m         stb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mInteractiveTB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3441\u001b[39m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   3442\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3443\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   3444\u001b[39m     \u001b[38;5;66;03m# In case formatting fails, fallback to Python's built-in formatting.\u001b[39;00m\n\u001b[32m   3445\u001b[39m     stb = traceback.format_exception(etype, evalue, tb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/IPython/core/ultratb.py:1182\u001b[39m, in \u001b[36mAutoFormattedTB.structured_traceback\u001b[39m\u001b[34m(self, etype, evalue, etb, tb_offset, context)\u001b[39m\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1181\u001b[39m     \u001b[38;5;28mself\u001b[39m.tb = etb\n\u001b[32m-> \u001b[39m\u001b[32m1182\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/IPython/core/ultratb.py:1053\u001b[39m, in \u001b[36mFormattedTB.structured_traceback\u001b[39m\u001b[34m(self, etype, evalue, etb, tb_offset, context)\u001b[39m\n\u001b[32m   1050\u001b[39m mode = \u001b[38;5;28mself\u001b[39m.mode\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose_modes:\n\u001b[32m   1052\u001b[39m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mDocs\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1057\u001b[39m     \u001b[38;5;66;03m# return DocTB\u001b[39;00m\n\u001b[32m   1058\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DocTB(\n\u001b[32m   1059\u001b[39m         theme_name=\u001b[38;5;28mself\u001b[39m._theme_name,\n\u001b[32m   1060\u001b[39m         call_pdb=\u001b[38;5;28mself\u001b[39m.call_pdb,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1068\u001b[39m         etype, evalue, etb, tb_offset, \u001b[32m1\u001b[39m\n\u001b[32m   1069\u001b[39m     )  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/IPython/core/ultratb.py:861\u001b[39m, in \u001b[36mVerboseTB.structured_traceback\u001b[39m\u001b[34m(self, etype, evalue, etb, tb_offset, context)\u001b[39m\n\u001b[32m    852\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstructured_traceback\u001b[39m(\n\u001b[32m    853\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    854\u001b[39m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    858\u001b[39m     context: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m,\n\u001b[32m    859\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    860\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m     formatted_exceptions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m        \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    865\u001b[39m     termsize = \u001b[38;5;28mmin\u001b[39m(\u001b[32m75\u001b[39m, get_terminal_size()[\u001b[32m0\u001b[39m])\n\u001b[32m    866\u001b[39m     theme = theme_table[\u001b[38;5;28mself\u001b[39m._theme_name]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/IPython/core/ultratb.py:773\u001b[39m, in \u001b[36mVerboseTB.format_exception_as_a_whole\u001b[39m\u001b[34m(self, etype, evalue, etb, context, tb_offset)\u001b[39m\n\u001b[32m    763\u001b[39m         frames.append(\n\u001b[32m    764\u001b[39m             theme_table[\u001b[38;5;28mself\u001b[39m._theme_name].format(\n\u001b[32m    765\u001b[39m                 [\n\u001b[32m   (...)\u001b[39m\u001b[32m    770\u001b[39m             )\n\u001b[32m    771\u001b[39m         )\n\u001b[32m    772\u001b[39m         skipped = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m     frames.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    774\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m skipped:\n\u001b[32m    775\u001b[39m     frames.append(\n\u001b[32m    776\u001b[39m         theme_table[\u001b[38;5;28mself\u001b[39m._theme_name].format(\n\u001b[32m    777\u001b[39m             [\n\u001b[32m   (...)\u001b[39m\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/IPython/core/ultratb.py:651\u001b[39m, in \u001b[36mVerboseTB.format_record\u001b[39m\u001b[34m(self, frame_info)\u001b[39m\n\u001b[32m    648\u001b[39m result += \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m call \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    649\u001b[39m result += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcall\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m result += theme_table[\u001b[38;5;28mself\u001b[39m._theme_name].format(\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     \u001b[43m_format_traceback_lines\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtheme_table\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_theme_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhas_colors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlvals_toks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m )\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/IPython/core/tbtools.py:99\u001b[39m, in \u001b[36m_format_traceback_lines\u001b[39m\u001b[34m(lines, theme, has_colors, lvals_toks)\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     98\u001b[39m lineno = stack_line.lineno\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m line = \u001b[43mstack_line\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpygmented\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_colors\u001b[49m\u001b[43m)\u001b[49m.rstrip(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m) + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stack_line.is_current:\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# This is the line with the error\u001b[39;00m\n\u001b[32m    102\u001b[39m     pad = numbers_width - \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mstr\u001b[39m(lineno))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/stack_data/core.py:391\u001b[39m, in \u001b[36mLine.render\u001b[39m\u001b[34m(self, markers, strip_leading_indent, pygmented, escape_html)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pygmented \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.frame_info.scope:\n\u001b[32m    390\u001b[39m     assert_(\u001b[38;5;129;01mnot\u001b[39;00m markers, \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot use pygmented with markers\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m     start_line, lines = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mframe_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_pygmented_scope_lines\u001b[49m\n\u001b[32m    392\u001b[39m     result = lines[\u001b[38;5;28mself\u001b[39m.lineno - start_line]\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m strip_leading_indent:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/stack_data/utils.py:145\u001b[39m, in \u001b[36mcached_property.cached_property_wrapper\u001b[39m\u001b[34m(self, obj, _cls)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m value = obj.\u001b[34m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m.func.\u001b[34m__name__\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/stack_data/core.py:824\u001b[39m, in \u001b[36mFrameInfo._pygmented_scope_lines\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    821\u001b[39m     ranges = []\n\u001b[32m    823\u001b[39m code = atext.get_text(scope)\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m lines = \u001b[43m_pygmented_with_ranges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m start_line = \u001b[38;5;28mself\u001b[39m.source.line_range(scope)[\u001b[32m0\u001b[39m]\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m start_line, lines\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/stack_data/utils.py:164\u001b[39m, in \u001b[36m_pygmented_with_ranges\u001b[39m\u001b[34m(formatter, code, ranges)\u001b[39m\n\u001b[32m    161\u001b[39m             length += \u001b[38;5;28mlen\u001b[39m(value)\n\u001b[32m    162\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m ttype, value\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m lexer = \u001b[43mMyLexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstripnl\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    166\u001b[39m     highlighted = pygments.highlight(code, lexer, formatter)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/lexer.py:660\u001b[39m, in \u001b[36mRegexLexerMeta.__call__\u001b[39m\u001b[34m(cls, *args, **kwds)\u001b[39m\n\u001b[32m    658\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m         \u001b[38;5;28mcls\u001b[39m._tokens = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_tokendef\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_tokendefs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m.\u001b[34m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, *args, **kwds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/lexer.py:599\u001b[39m, in \u001b[36mRegexLexerMeta.process_tokendef\u001b[39m\u001b[34m(cls, name, tokendefs)\u001b[39m\n\u001b[32m    597\u001b[39m tokendefs = tokendefs \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.tokens[name]\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(tokendefs):\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokendefs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m processed\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/lexer.py:563\u001b[39m, in \u001b[36mRegexLexerMeta._process_state\u001b[39m\u001b[34m(cls, unprocessed, processed, state)\u001b[39m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tdef, include):\n\u001b[32m    561\u001b[39m     \u001b[38;5;66;03m# it's a state reference\u001b[39;00m\n\u001b[32m    562\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m tdef != state, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcircular state reference \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     tokens.extend(\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43munprocessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m                                     \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtdef\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    565\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tdef, _inherit):\n\u001b[32m    567\u001b[39m     \u001b[38;5;66;03m# should be processed already, but may not in the case of:\u001b[39;00m\n\u001b[32m    568\u001b[39m     \u001b[38;5;66;03m# 1. the state has no counterpart in any parent\u001b[39;00m\n\u001b[32m    569\u001b[39m     \u001b[38;5;66;03m# 2. the state includes more than one 'inherit'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/lexer.py:563\u001b[39m, in \u001b[36mRegexLexerMeta._process_state\u001b[39m\u001b[34m(cls, unprocessed, processed, state)\u001b[39m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tdef, include):\n\u001b[32m    561\u001b[39m     \u001b[38;5;66;03m# it's a state reference\u001b[39;00m\n\u001b[32m    562\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m tdef != state, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcircular state reference \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     tokens.extend(\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43munprocessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m                                     \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtdef\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    565\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tdef, _inherit):\n\u001b[32m    567\u001b[39m     \u001b[38;5;66;03m# should be processed already, but may not in the case of:\u001b[39;00m\n\u001b[32m    568\u001b[39m     \u001b[38;5;66;03m# 1. the state has no counterpart in any parent\u001b[39;00m\n\u001b[32m    569\u001b[39m     \u001b[38;5;66;03m# 2. the state includes more than one 'inherit'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/lexer.py:579\u001b[39m, in \u001b[36mRegexLexerMeta._process_state\u001b[39m\u001b[34m(cls, unprocessed, processed, state)\u001b[39m\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tdef) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwrong rule def \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtdef\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m     rex = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_regex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtdef\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    581\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33muncompilable regex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtdef[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m in state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/lexer.py:507\u001b[39m, in \u001b[36mRegexLexerMeta._process_regex\u001b[39m\u001b[34m(cls, regex, rflags, state)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Preprocess the regular expression component of a token definition.\"\"\"\u001b[39;00m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(regex, Future):\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     regex = \u001b[43mregex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m re.compile(regex, rflags).match\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/lexer.py:495\u001b[39m, in \u001b[36mwords.get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregex_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/regexopt.py:91\u001b[39m, in \u001b[36mregex_opt\u001b[39m\u001b[34m(strings, prefix, suffix)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a compiled regex that matches any string in the given list.\u001b[39;00m\n\u001b[32m     84\u001b[39m \n\u001b[32m     85\u001b[39m \u001b[33;03mThe strings to match must be literal strings, not regexes.  They will be\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     88\u001b[39m \u001b[33;03m*prefix* and *suffix* are pre- and appended to the final regex.\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     90\u001b[39m strings = \u001b[38;5;28msorted\u001b[39m(strings)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prefix + \u001b[43mregex_opt_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m(\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m + suffix\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/regexopt.py:77\u001b[39m, in \u001b[36mregex_opt_inner\u001b[39m\u001b[34m(strings, open_paren)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m open_paren \\\n\u001b[32m     72\u001b[39m         + regex_opt_inner(\u001b[38;5;28msorted\u001b[39m(s[:-slen] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m strings), \u001b[33m'\u001b[39m\u001b[33m(?:\u001b[39m\u001b[33m'\u001b[39m) \\\n\u001b[32m     73\u001b[39m         + escape(suffix[::-\u001b[32m1\u001b[39m]) + close_paren\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# recurse on common 1-string prefixes\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# print '-> last resort'\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m open_paren + \\\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[33;43m'\u001b[39;49m\u001b[33;43m|\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregex_opt_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m             \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[32m     79\u001b[39m     + close_paren\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/regexopt.py:77\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m open_paren \\\n\u001b[32m     72\u001b[39m         + regex_opt_inner(\u001b[38;5;28msorted\u001b[39m(s[:-slen] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m strings), \u001b[33m'\u001b[39m\u001b[33m(?:\u001b[39m\u001b[33m'\u001b[39m) \\\n\u001b[32m     73\u001b[39m         + escape(suffix[::-\u001b[32m1\u001b[39m]) + close_paren\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# recurse on common 1-string prefixes\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# print '-> last resort'\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m open_paren + \\\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m|\u001b[39m\u001b[33m'\u001b[39m.join(\u001b[43mregex_opt_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m groupby(strings, \u001b[38;5;28;01mlambda\u001b[39;00m s: s[\u001b[32m0\u001b[39m] == first[\u001b[32m0\u001b[39m])) \\\n\u001b[32m     79\u001b[39m     + close_paren\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/regexopt.py:77\u001b[39m, in \u001b[36mregex_opt_inner\u001b[39m\u001b[34m(strings, open_paren)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m open_paren \\\n\u001b[32m     72\u001b[39m         + regex_opt_inner(\u001b[38;5;28msorted\u001b[39m(s[:-slen] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m strings), \u001b[33m'\u001b[39m\u001b[33m(?:\u001b[39m\u001b[33m'\u001b[39m) \\\n\u001b[32m     73\u001b[39m         + escape(suffix[::-\u001b[32m1\u001b[39m]) + close_paren\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# recurse on common 1-string prefixes\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# print '-> last resort'\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m open_paren + \\\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[33;43m'\u001b[39;49m\u001b[33;43m|\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregex_opt_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m             \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[32m     79\u001b[39m     + close_paren\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/regexopt.py:77\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m open_paren \\\n\u001b[32m     72\u001b[39m         + regex_opt_inner(\u001b[38;5;28msorted\u001b[39m(s[:-slen] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m strings), \u001b[33m'\u001b[39m\u001b[33m(?:\u001b[39m\u001b[33m'\u001b[39m) \\\n\u001b[32m     73\u001b[39m         + escape(suffix[::-\u001b[32m1\u001b[39m]) + close_paren\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# recurse on common 1-string prefixes\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# print '-> last resort'\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m open_paren + \\\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m|\u001b[39m\u001b[33m'\u001b[39m.join(\u001b[43mregex_opt_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m groupby(strings, \u001b[38;5;28;01mlambda\u001b[39;00m s: s[\u001b[32m0\u001b[39m] == first[\u001b[32m0\u001b[39m])) \\\n\u001b[32m     79\u001b[39m     + close_paren\n",
      "    \u001b[31m[... skipping similar frames: <genexpr> at line 77 (15 times), regex_opt_inner at line 77 (15 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/regexopt.py:77\u001b[39m, in \u001b[36mregex_opt_inner\u001b[39m\u001b[34m(strings, open_paren)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m open_paren \\\n\u001b[32m     72\u001b[39m         + regex_opt_inner(\u001b[38;5;28msorted\u001b[39m(s[:-slen] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m strings), \u001b[33m'\u001b[39m\u001b[33m(?:\u001b[39m\u001b[33m'\u001b[39m) \\\n\u001b[32m     73\u001b[39m         + escape(suffix[::-\u001b[32m1\u001b[39m]) + close_paren\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# recurse on common 1-string prefixes\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# print '-> last resort'\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m open_paren + \\\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[33;43m'\u001b[39;49m\u001b[33;43m|\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregex_opt_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m             \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[32m     79\u001b[39m     + close_paren\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/regexopt.py:77\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m open_paren \\\n\u001b[32m     72\u001b[39m         + regex_opt_inner(\u001b[38;5;28msorted\u001b[39m(s[:-slen] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m strings), \u001b[33m'\u001b[39m\u001b[33m(?:\u001b[39m\u001b[33m'\u001b[39m) \\\n\u001b[32m     73\u001b[39m         + escape(suffix[::-\u001b[32m1\u001b[39m]) + close_paren\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# recurse on common 1-string prefixes\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# print '-> last resort'\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m open_paren + \\\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m|\u001b[39m\u001b[33m'\u001b[39m.join(\u001b[43mregex_opt_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m groupby(strings, \u001b[38;5;28;01mlambda\u001b[39;00m s: s[\u001b[32m0\u001b[39m] == first[\u001b[32m0\u001b[39m])) \\\n\u001b[32m     79\u001b[39m     + close_paren\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/regexopt.py:63\u001b[39m, in \u001b[36mregex_opt_inner\u001b[39m\u001b[34m(strings, open_paren)\u001b[39m\n\u001b[32m     59\u001b[39m     plen = \u001b[38;5;28mlen\u001b[39m(prefix)\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# we have a prefix for all strings\u001b[39;00m\n\u001b[32m     61\u001b[39m     \u001b[38;5;66;03m# print '-> prefix:', prefix\u001b[39;00m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m open_paren + escape(prefix) \\\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         + \u001b[43mregex_opt_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mplen\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstrings\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m(?:\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \\\n\u001b[32m     64\u001b[39m         + close_paren\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# is there a suffix?\u001b[39;00m\n\u001b[32m     66\u001b[39m strings_rev = [s[::-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m strings]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/regexopt.py:77\u001b[39m, in \u001b[36mregex_opt_inner\u001b[39m\u001b[34m(strings, open_paren)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m open_paren \\\n\u001b[32m     72\u001b[39m         + regex_opt_inner(\u001b[38;5;28msorted\u001b[39m(s[:-slen] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m strings), \u001b[33m'\u001b[39m\u001b[33m(?:\u001b[39m\u001b[33m'\u001b[39m) \\\n\u001b[32m     73\u001b[39m         + escape(suffix[::-\u001b[32m1\u001b[39m]) + close_paren\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# recurse on common 1-string prefixes\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# print '-> last resort'\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m open_paren + \\\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[33;43m'\u001b[39;49m\u001b[33;43m|\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregex_opt_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m             \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[32m     79\u001b[39m     + close_paren\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/regexopt.py:77\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m open_paren \\\n\u001b[32m     72\u001b[39m         + regex_opt_inner(\u001b[38;5;28msorted\u001b[39m(s[:-slen] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m strings), \u001b[33m'\u001b[39m\u001b[33m(?:\u001b[39m\u001b[33m'\u001b[39m) \\\n\u001b[32m     73\u001b[39m         + escape(suffix[::-\u001b[32m1\u001b[39m]) + close_paren\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# recurse on common 1-string prefixes\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# print '-> last resort'\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m open_paren + \\\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m|\u001b[39m\u001b[33m'\u001b[39m.join(\u001b[43mregex_opt_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m groupby(strings, \u001b[38;5;28;01mlambda\u001b[39;00m s: s[\u001b[32m0\u001b[39m] == first[\u001b[32m0\u001b[39m])) \\\n\u001b[32m     79\u001b[39m     + close_paren\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/regexopt.py:63\u001b[39m, in \u001b[36mregex_opt_inner\u001b[39m\u001b[34m(strings, open_paren)\u001b[39m\n\u001b[32m     59\u001b[39m     plen = \u001b[38;5;28mlen\u001b[39m(prefix)\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# we have a prefix for all strings\u001b[39;00m\n\u001b[32m     61\u001b[39m     \u001b[38;5;66;03m# print '-> prefix:', prefix\u001b[39;00m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m open_paren + escape(prefix) \\\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         + \u001b[43mregex_opt_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mplen\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstrings\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m(?:\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \\\n\u001b[32m     64\u001b[39m         + close_paren\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# is there a suffix?\u001b[39;00m\n\u001b[32m     66\u001b[39m strings_rev = [s[::-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m strings]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/regexopt.py:77\u001b[39m, in \u001b[36mregex_opt_inner\u001b[39m\u001b[34m(strings, open_paren)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m open_paren \\\n\u001b[32m     72\u001b[39m         + regex_opt_inner(\u001b[38;5;28msorted\u001b[39m(s[:-slen] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m strings), \u001b[33m'\u001b[39m\u001b[33m(?:\u001b[39m\u001b[33m'\u001b[39m) \\\n\u001b[32m     73\u001b[39m         + escape(suffix[::-\u001b[32m1\u001b[39m]) + close_paren\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# recurse on common 1-string prefixes\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# print '-> last resort'\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m open_paren + \\\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[33;43m'\u001b[39;49m\u001b[33;43m|\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregex_opt_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m             \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[32m     79\u001b[39m     + close_paren\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/regexopt.py:77\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m open_paren \\\n\u001b[32m     72\u001b[39m         + regex_opt_inner(\u001b[38;5;28msorted\u001b[39m(s[:-slen] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m strings), \u001b[33m'\u001b[39m\u001b[33m(?:\u001b[39m\u001b[33m'\u001b[39m) \\\n\u001b[32m     73\u001b[39m         + escape(suffix[::-\u001b[32m1\u001b[39m]) + close_paren\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# recurse on common 1-string prefixes\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# print '-> last resort'\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m open_paren + \\\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m|\u001b[39m\u001b[33m'\u001b[39m.join(\u001b[43mregex_opt_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m groupby(strings, \u001b[38;5;28;01mlambda\u001b[39;00m s: s[\u001b[32m0\u001b[39m] == first[\u001b[32m0\u001b[39m])) \\\n\u001b[32m     79\u001b[39m     + close_paren\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pygments/regexopt.py:36\u001b[39m, in \u001b[36mregex_opt_inner\u001b[39m\u001b[34m(strings, open_paren)\u001b[39m\n\u001b[32m     33\u001b[39m first = strings[\u001b[32m0\u001b[39m]\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(strings) == \u001b[32m1\u001b[39m:\n\u001b[32m     35\u001b[39m     \u001b[38;5;66;03m# print '-> only 1 string'\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m open_paren + \u001b[43mescape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m)\u001b[49m + close_paren\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m first:\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# print '-> first string empty'\u001b[39;00m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m open_paren + regex_opt_inner(strings[\u001b[32m1\u001b[39m:], \u001b[33m'\u001b[39m\u001b[33m(?:\u001b[39m\u001b[33m'\u001b[39m) \\\n\u001b[32m     40\u001b[39m         + \u001b[33m'\u001b[39m\u001b[33m?\u001b[39m\u001b[33m'\u001b[39m + close_paren\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/re/__init__.py:260\u001b[39m, in \u001b[36mescape\u001b[39m\u001b[34m(pattern)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    257\u001b[39m \u001b[33;03mEscape special characters in a string.\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pattern, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpattern\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_special_chars_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    262\u001b[39m     pattern = \u001b[38;5;28mstr\u001b[39m(pattern, \u001b[33m'\u001b[39m\u001b[33mlatin1\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Just to debug if needed\n",
    "\n",
    "model.cpu()\n",
    "batch = next(iter(train_dataloader))\n",
    "batch = {k: v.cpu() for k, v in batch.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(**batch)\n",
    "print(out.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "618535fd-9c56-4487-a949-d33055323ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8987"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75bdd594-0b48-432c-ad60-4bb693755177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11984"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader) * epochs\n",
    "# == (len(tokenized_sp_small[\"train\"]) // batch_size) * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "754e32a3-a1ee-4b58-87e0-e1e321b0eeab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_training_steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m progress_bar = tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[43mnum_training_steps\u001b[49m))\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_train_epochs):\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[32m      9\u001b[39m     model.train()\n",
      "\u001b[31mNameError\u001b[39m: name 'num_training_steps' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            # If we did not pad to max length, we need to pad the labels too\n",
    "            labels = accelerator.pad_across_processes(\n",
    "                batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "            labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "            # Replace -100 in the labels as we can't decode them\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "            decoded_preds = tokenizer.batch_decode(\n",
    "                generated_tokens, skip_special_tokens=True\n",
    "            )\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            decoded_preds, decoded_labels = postprocess_text(\n",
    "                decoded_preds, decoded_labels\n",
    "            )\n",
    "\n",
    "            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    result = rouge_score.compute()\n",
    "    # Extract the median ROUGE scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    print(f\"Epoch {epoch}:\", result)\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        unwrapped_model.push_to_hub( # Is it model.push_to_hub??\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017047e8-66dc-41e3-a0bc-f4cd34e54064",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "- LoRA\n",
    "- Quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b7cbf51-8574-4c6b-89d9-ed9c59027e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'abstract', 'section_names', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = load_from_disk(\"/workspace/scientific_papers_hf\")\n",
    "tokenized_sp_small = load_from_disk(\"/workspace/tokenized_sp_small\")\n",
    "\n",
    "tss = tokenized_sp_small[\"train\"].select(range(100))\n",
    "\n",
    "tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33af7b84-9b13-4ac5-bfc6-eb9fdcc3f88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from datasets import load_from_disk\n",
    "from aiohttp import ClientTimeout\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "# from accelerate.utils import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import pipeline, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import numpy as np\n",
    "import rouge_score\n",
    "import evaluate\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "    \n",
    "\n",
    "def train_ddp_accelerate():\n",
    "\n",
    "    output_dir = \"led-abstract-gen\"\n",
    "    max_target_length = 256\n",
    "\n",
    "    # Load tokenizer\n",
    "    \n",
    "    checkpoint = \"allenai/led-base-16384\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    # Load dataset\n",
    "    \n",
    "    sp = load_from_disk(\"/workspace/scientific_papers_hf\")\n",
    "    tokenized_sp_small = load_from_disk(\"/workspace/tokenized_sp_small\")\n",
    "\n",
    "    # 3 is good for fine-tuning, 6 is good for LoRA, 8 is good for quantization, 8 is good for QLoRA\n",
    "    # Note: performance is worse when fine-tuning quantized models than full models, \n",
    "    #   we should use QLoRA in that case. Even then, it will be slower than plain LoRA\n",
    "    #   (it is due to extra computation for quant/dequant and custom, less efficient kernels)\n",
    "    batch_size = 6 # I needed to reduce it from 6 for multi-gpu. I guess there is some overhead.\n",
    "    epochs = 4\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, pad_to_multiple_of=1024)\n",
    "    tokenized_sp_small = tokenized_sp_small.remove_columns(sp[\"train\"].column_names)\n",
    "    \n",
    "    train_dataloader = DataLoader(tokenized_sp_small[\"train\"], batch_size=batch_size, collate_fn=data_collator, shuffle=True)\n",
    "    \n",
    "    eval_dataloader = DataLoader(tokenized_sp_small[\"validation\"], batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "    # Accelerator\n",
    "\n",
    "    # ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "    accelerator = Accelerator(\n",
    "                              mixed_precision=\"fp16\", # remove for quantized models\n",
    "                              # kwargs_handlers=[ddp_kwargs], # remove for LoRA\n",
    "                              # gradient_accumulation_steps=gradient_accumulation_steps\n",
    "                             )\n",
    "\n",
    "    # Quantize and build model\n",
    "\n",
    "    # quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    \n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, \n",
    "                                                  # quantization_config=quantization_config,\n",
    "                                                  # device_map={\"\": accelerator.local_process_index} # needed for quant?\n",
    "                                                 )\n",
    "\n",
    "    # model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # LoRA\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=128,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"out_proj\", \"k_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    # Build optimizer\n",
    "    \n",
    "    optimizer = Adafactor(model.parameters(), scale_parameter=False, relative_step=False, lr=1e-3) # Not sure about these settings\n",
    "    # lr_scheduler = AdafactorSchedule(optimizer) # Doesn't work well, seems to be decaying the lr too quickly and unstabilizing it\n",
    "\n",
    "    # Prepare\n",
    "\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
    "\n",
    "    # Configure evaluator\n",
    "\n",
    "    rouge_score = evaluate.load(\"rouge\")\n",
    "\n",
    "    # Progress bar\n",
    "    \n",
    "    # num_update_steps_per_epoch = len(train_dataloader)\n",
    "    # num_training_steps = epochs * num_update_steps_per_epoch\n",
    "    \n",
    "    # progress_bar = tqdm(range(num_training_steps), disable=not accelerator.is_local_main_process)\n",
    "\n",
    "    # Train\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss # Cross-entropy by default for seq2seq\n",
    "            accelerator.backward(loss)\n",
    "    \n",
    "            optimizer.step()\n",
    "            # lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if accelerator.is_main_process and step % 10 == 0:\n",
    "                elapsed = time.time()-start_time\n",
    "                el_min, el_sec = divmod(int(elapsed), 60)\n",
    "                estimated_total = elapsed / (step + 1) * len(train_dataloader)\n",
    "                rem_min, rem_sec = divmod(int(estimated_total - elapsed), 60)\n",
    "                print(\n",
    "                    f\"{step + 1}/{len(train_dataloader)} | epoch {epoch + 1}/{epochs} | \"\n",
    "                    f\"elapsed: {el_min}m{el_sec}s | per example: {elapsed/(step+1)/batch_size:.2f} | \"\n",
    "                    f\"remaining: {rem_min}m{rem_sec}s | loss: {loss.item():.2f}\"\n",
    "                )\n",
    "    \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "                generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    max_new_tokens=max_target_length\n",
    "                )\n",
    "    \n",
    "                generated_tokens = accelerator.pad_across_processes(\n",
    "                    generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "                )\n",
    "                labels = batch[\"labels\"]\n",
    "    \n",
    "                # If we did not pad to max length, we need to pad the labels too\n",
    "                labels = accelerator.pad_across_processes(\n",
    "                    batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id\n",
    "                )\n",
    "    \n",
    "                generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "                labels = accelerator.gather(labels).cpu().numpy()\n",
    "    \n",
    "                # Replace -100 in the labels as we can't decode them\n",
    "                labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "                if isinstance(generated_tokens, tuple):\n",
    "                    generated_tokens = generated_tokens[0]\n",
    "                decoded_preds = tokenizer.batch_decode(\n",
    "                    generated_tokens, skip_special_tokens=True\n",
    "                )\n",
    "                decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "                decoded_preds, decoded_labels = postprocess_text(\n",
    "                    decoded_preds, decoded_labels\n",
    "                )\n",
    "    \n",
    "                rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "        # Compute metrics\n",
    "        if accelerator.is_main_process:\n",
    "            result = rouge_score.compute()\n",
    "            # Extract the median ROUGE scores\n",
    "            result = {k: round(float(v) * 100, 4) for k, v in result.items()}\n",
    "            print(f\"Epoch {epoch}:\", result)\n",
    "    \n",
    "        # Save and upload\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save) # Run in all processes as model is sharded\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(output_dir) # Run only on main process as tokenizer is common\n",
    "            unwrapped_model.push_to_hub(\n",
    "                repo_id=\"miguelgrc/led-abstract-gen\",\n",
    "                commit_message=f\"Training in progress epoch {epoch + 1}\", \n",
    "                blocking=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91bd4015-57e0-47e6-878b-e34e351504d8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,718,592 || all params: 166,563,072 || trainable%: 2.8329\n",
      "1/34 (epoch 1/2) (elapsed: 0.0m1.244530439376831s) (per example: 0.21) (remaining: 0m41s) (loss: 3.648803234100342\n",
      "11/34 (epoch 1/2) (elapsed: 0.0m10.207772254943848s) (per example: 0.15) (remaining: 0m21s) (loss: 5.167680740356445\n",
      "21/34 (epoch 1/2) (elapsed: 0.0m18.382553815841675s) (per example: 0.15) (remaining: 0m11s) (loss: 4.244979381561279\n",
      "31/34 (epoch 1/2) (elapsed: 0.0m26.86331820487976s) (per example: 0.14) (remaining: 0m2s) (loss: 4.5241475105285645\n",
      "Epoch 0: {'rouge1': 21.5082, 'rouge2': 5.2163, 'rougeL': 14.5929, 'rougeLsum': 18.8433}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da442d79d0e141399535dcb296fd66f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/18.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/34 (epoch 2/2) (elapsed: 0.0m0.8697819709777832s) (per example: 0.14) (remaining: 0m28s) (loss: 4.71817684173584\n",
      "11/34 (epoch 2/2) (elapsed: 0.0m9.783159255981445s) (per example: 0.15) (remaining: 0m20s) (loss: 4.114262104034424\n",
      "21/34 (epoch 2/2) (elapsed: 0.0m17.93355369567871s) (per example: 0.14) (remaining: 0m11s) (loss: 3.8215792179107666\n",
      "31/34 (epoch 2/2) (elapsed: 0.0m26.09733819961548s) (per example: 0.14) (remaining: 0m2s) (loss: 4.694792747497559\n",
      "Epoch 1: {'rouge1': 21.5082, 'rouge2': 5.2163, 'rougeL': 14.5929, 'rougeLsum': 18.8433}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    }
   ],
   "source": [
    "# To debug errors in one GPU\n",
    "train_ddp_accelerate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cd120e-0e46-411d-a5db-c274401a9783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n",
      "trainable params: 4,718,592 || all params: 166,563,072 || trainable%: 2.8329\n",
      "1/2996 | epoch 1/4) | elapsed: 0m1s | per example: 0.20 | remaining: 60m47s | loss: 4.80\n",
      "11/2996 | epoch 1/4) | elapsed: 0m10s | per example: 0.16 | remaining: 47m1s | loss: 2.87\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(train_ddp_accelerate, args=(), num_processes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b9e08e-2562-4b1c-9938-2c500501ded5",
   "metadata": {},
   "source": [
    "Fine-tuning: 0.35s per example\n",
    "LoRA: 0.15 per example\n",
    "8-bit quant: 0.69s per example\n",
    "QLoRA: 0.74s per example\n",
    "\n",
    "Advantages of LoRA: less memory, allowing for bigger batch sizes and therefore more speed. Also, the trained model is easier to store (the relatively small adapters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e8241b-5675-4d5a-a4c3-fc5203e14505",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# GPU monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "044188ec-4973-4748-b5dd-2e6a60a5f54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.00 MB (())\n",
      "p: 0.00 MB ((768,))\n",
      "model (model): 617.39 MB\n",
      "\n",
      "Total GPU memory used by variables: 617.39 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "total_bytes = 0\n",
    "\n",
    "for var_name, var_val in list(globals().items()):  # <-- make a list copy\n",
    "    if torch.is_tensor(var_val) and var_val.is_cuda:\n",
    "        size_bytes = var_val.element_size() * var_val.nelement()\n",
    "        total_bytes += size_bytes\n",
    "        print(f\"{var_name}: {size_bytes/1024/1024:.2f} MB ({tuple(var_val.shape)})\")\n",
    "    elif hasattr(var_val, 'parameters'):\n",
    "        # For models, sum all parameters on CUDA\n",
    "        model_bytes = 0\n",
    "        for p in var_val.parameters():\n",
    "            if p.is_cuda:\n",
    "                model_bytes += p.element_size() * p.nelement()\n",
    "        if model_bytes > 0:\n",
    "            total_bytes += model_bytes\n",
    "            print(f\"{var_name} (model): {model_bytes/1024/1024:.2f} MB\")\n",
    "\n",
    "print(f\"\\nTotal GPU memory used by variables: {total_bytes/1024/1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5abcbe77-f712-4be8-8c62-4e40fe89da0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 3            |        cudaMalloc retries: 8         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  31117 MiB |  31501 MiB | 449855 MiB | 418738 MiB |\n",
      "|       from large pool |  31110 MiB |  31496 MiB | 449621 MiB | 418510 MiB |\n",
      "|       from small pool |      6 MiB |      7 MiB |    234 MiB |    227 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  31117 MiB |  31501 MiB | 449855 MiB | 418738 MiB |\n",
      "|       from large pool |  31110 MiB |  31496 MiB | 449621 MiB | 418510 MiB |\n",
      "|       from small pool |      6 MiB |      7 MiB |    234 MiB |    227 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  31054 MiB |  31448 MiB | 449522 MiB | 418468 MiB |\n",
      "|       from large pool |  31047 MiB |  31443 MiB | 449288 MiB | 418240 MiB |\n",
      "|       from small pool |      6 MiB |      7 MiB |    234 MiB |    227 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  31746 MiB |  31948 MiB |  66842 MiB |  35096 MiB |\n",
      "|       from large pool |  31738 MiB |  31940 MiB |  66830 MiB |  35092 MiB |\n",
      "|       from small pool |      8 MiB |      8 MiB |     12 MiB |      4 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 643604 KiB |   8938 MiB | 213359 MiB | 212731 MiB |\n",
      "|       from large pool | 642381 KiB |   8934 MiB | 213103 MiB | 212476 MiB |\n",
      "|       from small pool |   1223 KiB |      4 MiB |    256 MiB |    255 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1066    |    1069    |    5750    |    4684    |\n",
      "|       from large pool |     540    |     540    |    3820    |    3280    |\n",
      "|       from small pool |     526    |     536    |    1930    |    1404    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1066    |    1069    |    5750    |    4684    |\n",
      "|       from large pool |     540    |     540    |    3820    |    3280    |\n",
      "|       from small pool |     526    |     536    |    1930    |    1404    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      64    |      72    |     110    |      46    |\n",
      "|       from large pool |      60    |      68    |     104    |      44    |\n",
      "|       from small pool |       4    |       4    |       6    |       2    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      53    |      98    |    2477    |    2424    |\n",
      "|       from large pool |      27    |      66    |    1824    |    1797    |\n",
      "|       from small pool |      26    |      37    |     653    |     627    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7894"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, gc\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "print(torch.cuda.memory_summary())\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77e112-018b-41f6-b338-5e7b3a6f9360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.to('cuda')\n",
    "# # model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1dd17cf6-6c99-4281-8579-83be58fa2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bUdUtJG-pT1p",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bUdUtJG-pT1p",
    "outputId": "04cf3455-0ca7-42d6-abc9-63ac1ac7579f"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c103400e-100b-4bf3-a067-2569aca5b018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1578152\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-compute-apps=pid --format=csv,noheader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "949b4e92-d4a2-4b2b-a15e-f254fe4769b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID          PID    PPID  C STIME TTY          TIME CMD\n"
     ]
    }
   ],
   "source": [
    "!ps -fp 1578152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "Z8bZ68U4ptBP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z8bZ68U4ptBP",
    "outputId": "be64d14c-56bc-4c80-a70a-80518bf7071b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numba\n",
      "  Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba)\n",
      "  Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting numpy<2.3,>=1.24 (from numba)\n",
      "  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, llvmlite, numba\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 2.3.3\n",
      "\u001b[2K    Uninstalling numpy-2.3.3:\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.3\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [numba]32m2/3\u001b[0m [numba]te]\n",
      "\u001b[1A\u001b[2KSuccessfully installed llvmlite-0.44.0 numba-0.61.2 numpy-2.2.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 2.2 or less. Got NumPy 2.3.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Clear VRAM\u001b[39;00m\n\u001b[32m      3\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[38;5;132;01m{sys.executable}\u001b[39;00m\u001b[33m -m pip install numba\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cuda\n\u001b[32m      6\u001b[39m device = cuda.get_current_device()\n\u001b[32m      7\u001b[39m device.reset()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/numba/__init__.py:59\u001b[39m\n\u001b[32m     54\u001b[39m             msg = (\u001b[33m\"\u001b[39m\u001b[33mNumba requires SciPy version 1.0 or greater. Got SciPy \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     55\u001b[39m                    \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscipy.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43m_ensure_critical_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# END DO NOT MOVE\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# ---------------------- WARNING WARNING WARNING ----------------------------\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_versions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/numba/__init__.py:45\u001b[39m, in \u001b[36m_ensure_critical_deps\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m numpy_version > (\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m):\n\u001b[32m     43\u001b[39m     msg = (\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumba needs NumPy 2.2 or less. Got NumPy \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     44\u001b[39m            \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Numba needs NumPy 2.2 or less. Got NumPy 2.3."
     ]
    }
   ],
   "source": [
    "# Clear VRAM\n",
    "\n",
    "# !{sys.executable} -m pip install numba\n",
    "\n",
    "from numba import cuda\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "L9gTSZ2Hp9oa",
   "metadata": {
    "id": "L9gTSZ2Hp9oa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-ml-py3\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nvidia-ml-py3\n",
      "\u001b[33m  DEPRECATION: Building 'nvidia-ml-py3' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'nvidia-ml-py3'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19208 sha256=40c61b65341140e188272b1cd22a94917c95e4ee1793202b4837209912523c22\n",
      "  Stored in directory: /root/.cache/pip/wheels/6e/65/79/33dee66cba26e8204801916dfee7481bccfd22905ebb841fe5\n",
      "Successfully built nvidia-ml-py3\n",
      "Installing collected packages: nvidia-ml-py3\n",
      "Successfully installed nvidia-ml-py3-7.352.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1c585333-3451-486d-b610-834eb5ceeeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3d139d37-0f8b-4c0e-86ba-3aca40893d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 29565 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4201dae4-caa5-4c84-b02f-0236016ad269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "69807d89-dd61-4ca4-86a4-d4237d2d059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Start recording memory snapshot history\n",
    "torch.cuda.memory._record_memory_history(max_entries=100000)\n",
    "\n",
    "model = nn.Linear(10_000, 50_000, device =\"cuda\")\n",
    "for _ in range(3):\n",
    "    inputs = torch.randn(5_000, 10_000, device=\"cuda\")\n",
    "    outputs = model(inputs)\n",
    "\n",
    "# Dump memory snapshot history to a file and stop recording\n",
    "torch.cuda.memory._dump_snapshot(\"profile.pkl\")\n",
    "torch.cuda.memory._record_memory_history(enabled=None)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "074f48956e9b4d6d953969ca353f795d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2210239dd2224be58b81e293c674c12f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f25d707a1108402db167ac67361fcc55",
      "max": 35,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2a5e7e178a8a43c7b5165897cad0a610",
      "value": 35
     }
    },
    "2a5e7e178a8a43c7b5165897cad0a610": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3d2ffdf5f59a47fbb4435c4b8e918261": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f11be13010749e98d23ad701cb9d7fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a220894313684e1dbd0e18b37469d2ab",
      "placeholder": "​",
      "style": "IPY_MODEL_b90789bc945b4417beaa19294b8a9aab",
      "value": " 2/2 [00:38&lt;00:00, 18.02s/it]"
     }
    },
    "52267788a93a493e8b2b83cf6ed30e95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd545979c96e4392b0693f910f62ade2",
      "placeholder": "​",
      "style": "IPY_MODEL_c4bc7f30f36e4186bbcb44ae7681cd20",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "904a256fd5ab473d97a3a4a5c0ea158c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_be24a2d912dd4f73a6eac62aaaa4b650",
       "IPY_MODEL_2210239dd2224be58b81e293c674c12f",
       "IPY_MODEL_fd66ac3a4e2c4c75a8a06f6acfe1662f"
      ],
      "layout": "IPY_MODEL_95599445173345338dfcf9adc8c4cee0"
     }
    },
    "95599445173345338dfcf9adc8c4cee0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9685442264e440f0bdc28f4782941587": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ded3180a8f81466c9ebf03c3f6b369d4",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a5be1b54a6b048c09dd37d3fa7f14f72",
      "value": 2
     }
    },
    "a220894313684e1dbd0e18b37469d2ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5be1b54a6b048c09dd37d3fa7f14f72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a9412c0c1980445bbb38ed40b4c7dbed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b90789bc945b4417beaa19294b8a9aab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be24a2d912dd4f73a6eac62aaaa4b650": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d2ffdf5f59a47fbb4435c4b8e918261",
      "placeholder": "​",
      "style": "IPY_MODEL_f670977056df4ac38d5f698d9ac26764",
      "value": "Loading dataset from disk: 100%"
     }
    },
    "c4bc7f30f36e4186bbcb44ae7681cd20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dd545979c96e4392b0693f910f62ade2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ded3180a8f81466c9ebf03c3f6b369d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f25d707a1108402db167ac67361fcc55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f36d392e748347d5acb87be1bc8b7186": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3b3901214d346f88c1a91e7e38fe660": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_52267788a93a493e8b2b83cf6ed30e95",
       "IPY_MODEL_9685442264e440f0bdc28f4782941587",
       "IPY_MODEL_4f11be13010749e98d23ad701cb9d7fc"
      ],
      "layout": "IPY_MODEL_a9412c0c1980445bbb38ed40b4c7dbed"
     }
    },
    "f670977056df4ac38d5f698d9ac26764": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd66ac3a4e2c4c75a8a06f6acfe1662f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_074f48956e9b4d6d953969ca353f795d",
      "placeholder": "​",
      "style": "IPY_MODEL_f36d392e748347d5acb87be1bc8b7186",
      "value": " 35/35 [02:17&lt;00:00,  3.88s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
